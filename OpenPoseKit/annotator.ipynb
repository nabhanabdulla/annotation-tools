{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPoseKit Step 2 - Annotate your data\n",
    "\n",
    "This is step 2 of the example notebooks for using DeepPoseKit. This notebook shows you how to annotate your training data with user-defined keypoints using the saved data from step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already installed DeepPoseKit you can run the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to download the example data into your home directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancements\n",
    "\n",
    "**This notebook builds on the template provided [here] to**\n",
    "\n",
    "1. First convert annotation from pre-trained [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) model to the required format for the DeepPoseKit toolkit \n",
    "\n",
    "2. Cumulate raw images and annotations together to HDF5 format of required batch size to reduce memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the annotations for a person in an image copied to another person in another image. Say, annotator's 25th frame highlights an annotation - copy this annotation to maybe the next image where the person is in the same (29) position - maybe detect a key input such as - ord('c') ord('2') ord('5') ord(' ') ord('2') ord('9') ord(' ')\n",
    "\n",
    "2. Extend to 25 annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/jgraving/deepposekit-data {HOME + '/deepposekit-data'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "# output should be (num_images, num_coords, 2)\n",
    "def openpose2deeppose(keypoint, num_points=19, footpoints=False):\n",
    "    '''\n",
    "    Returns individual person pose keypoints from openpose json keypoints\n",
    "    \n",
    "    openpose keypoints are \n",
    "        - 25*(x, y, confidence) with footpoints\n",
    "        - 19*(x, y, confidence) w/o footpoints\n",
    "    \n",
    "    footpoints - if to include heel, bigtoe, smalltoe\n",
    "    '''\n",
    "    poses = [data['pose_keypoints_2d'] for data in keypoint['people']]\n",
    "    only_coords = [[[pose[3*i], pose[3*i+1]] for i in range(num_points)] for pose in poses]\n",
    "    \n",
    "    # solution for - images with no annotations doesn't show up in annotator\n",
    "    if only_coords == []:\n",
    "        only_coords = [[[0, 0] for i in range(num_points)]]\n",
    "    \n",
    "    return only_coords\n",
    "\n",
    "\n",
    "def get_annotations_image(annotations, person_count, filenames):\n",
    "    # get list of indices repeated according to person_count\n",
    "    indices = np.repeat(np.arange(len(person_count)), person_count)\n",
    "#     print(len(indices), len(annotations))\n",
    "    assert len(filenames) == len(person_count)\n",
    "    assert len(annotations) == len(indices)\n",
    "    \n",
    "    annotations_consolidated = defaultdict(list)\n",
    "    \n",
    "    for index, annotation in zip(indices, annotations):\n",
    "        annotations_consolidated[filenames[index]].append(list(annotation))\n",
    "    \n",
    "    \n",
    "    return annotations_consolidated\n",
    "\n",
    "def store_annotations_json(filenames, annotations):\n",
    "    # get list of indices repeated according to person_count\n",
    "#     indices = np.repeat(np.arange(len(person_count)), person_count)\n",
    "    #     print(len(indices), len(annotations))\n",
    "    assert len(filenames) == len(annotations)\n",
    "\n",
    "    output_dict = {\"name\": '', \"version\":1.3, \"people\": []}\n",
    "\n",
    "    set_names = {}\n",
    "    # create a single json file named after the image name\n",
    "    # for each annotation of an image add the annotation to \"people\" using people_dict template\n",
    "    for name, annotation in zip(filenames, annotations):\n",
    "        people_dict = {\"person_id\":[-1],\"pose_keypoints_2d\":[],\"face_keypoints_2d\":[],\"hand_left_keypoints_2d\":[],\"hand_right_keypoints_2d\":[],\"pose_keypoints_3d\":[],\"face_keypoints_3d\":[],\"hand_left_keypoints_3d\":[],\"hand_right_keypoints_3d\":[]}\n",
    "\n",
    "        # To avoid error\n",
    "        # json.dumps(dicts) - TypeError: Object of type bytes_ is not JSON serializable\n",
    "        name = name.decode('utf-8')\n",
    "    #     print(name)\n",
    "        if name not in set_names.keys():\n",
    "\n",
    "            new_output_dict = deepcopy(output_dict)\n",
    "\n",
    "            new_output_dict['name'] = name\n",
    "\n",
    "\n",
    "            # add 25 annotations\n",
    "            annotations_mod = np.array(annotation).tolist()\n",
    "            people_dict[\"pose_keypoints_2d\"] = annotations_mod\n",
    "#             print(len(annotations), annotations[0].shape)\n",
    "\n",
    "            new_output_dict['people'].append(people_dict)\n",
    "\n",
    "            set_names[name] = new_output_dict\n",
    "        else:\n",
    "            curr_output_dict = set_names[name]\n",
    "\n",
    "            people_dict[\"pose_keypoints_2d\"] = np.array(annotation).tolist()\n",
    "            curr_output_dict['people'].append(people_dict)\n",
    "\n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "    # storing ndarrays as json- https://stackoverflow.com/a/47626762/9734484\n",
    "    for name, dicts in set_names.items():\n",
    "        name = name.replace('.jpg', '')\n",
    "        name = name.replace('.png', '')\n",
    "        \n",
    "        with open(f'json/{name}.json', 'w') as fp:\n",
    "            json.dump(dicts, fp, cls=NumpyEncoder, indent=2)\n",
    "\n",
    "\n",
    "    print('Store complete')\n",
    "    \n",
    "# \"output/openpose_annotations_0.h5\"\n",
    "def annotation_from_h5py2json(path):\n",
    "    print(path)\n",
    "    with h5py.File(path, 'r') as hf:\n",
    "        filenames = list(hf['image_path'])\n",
    "        annotations = list(hf['annotations'])\n",
    "        \n",
    "#     print(len(annotations))\n",
    "#     print(len(filenames))\n",
    "    store_annotations_json(filenames, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dicts['people'])\n",
    "# # dicts['people'][1]\n",
    "# annotation_string = json.dumps(dicts, indent=2)\n",
    "# with open(f'json/{name}.json', 'w') as fp:\n",
    "#     fp.write(annotation_string)\n",
    "# json.dumps(dicts['name'])\n",
    "# json.dumps(dicts['people'])\n",
    "# np.array(annotations).shape\n",
    "# dicts['people'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get image and keypoint, names and paths - change IMAGE_DIR and KEY_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image directory - raw images - not the openpose image output having keypoints drawn on it\n",
    "IMAGE_DIR = '/home/nabhanpv/Desktop/projects/mlabs/data/3_23/raw'\n",
    "\n",
    "# keypoint directory - json keypoint output from openpose\n",
    "KEY_DIR = '/home/nabhanpv/Desktop/projects/mlabs/data/3_23/output/json_output'\n",
    "\n",
    "\n",
    "# image file names sorted by date\n",
    "# eg: ch03_20190823170038_775.jpg, ch03_20190823170038_5275\n",
    "image_files = [filename for filename in sorted(os.listdir(IMAGE_DIR), \n",
    "                                               key=lambda x: \n",
    "                                               (int(x.split('_')[1])*10 + int(x.split('_')[2].split('.')[0])))]\n",
    "\n",
    "# absolute paths to the images\n",
    "image_paths = [os.path.join(IMAGE_DIR, filename) for filename in image_files]\n",
    "\n",
    "# keypoint file names corresponding to the image files\n",
    "# eg: ch03_20190823170038_775_keypoints\n",
    "keypoint_files = [file.replace('.jpg', '_keypoints.json') for file in image_files]\n",
    "\n",
    "# path to the keypoint files\n",
    "keypoint_paths = [os.path.join(KEY_DIR, filename) for filename in keypoint_files]\n",
    "\n",
    "# loop over the keypoint files and save as dict\n",
    "keypoint_array = []\n",
    "for path in keypoint_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        keypoint_data = json.load(f)\n",
    "        keypoint_array.append(keypoint_data)\n",
    "\n",
    "# change list of lists to np.ndarray\n",
    "keypoint_array = np.array(keypoint_array)\n",
    "\n",
    "# store pose keypoints only\n",
    "keypoints_deeppose = []\n",
    "\n",
    "# keep count of persons in each image\n",
    "person_count = []\n",
    "for keypoint in keypoint_array:\n",
    "    # extract pose keypoints\n",
    "    kpoints = openpose2deeppose(keypoint)\n",
    "    \n",
    "    # store count of person in each image\n",
    "    n_persons = len(kpoints)\n",
    "    person_count.append(n_persons)\n",
    "\n",
    "    # Add as multiple elements rather than a list\n",
    "    keypoints_deeppose += kpoints\n",
    "\n",
    "# cumulative sum of person count\n",
    "person_count_cum = np.cumsum(person_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 6, 2, 4, 6, 5, 3, 4, 4, 3, 5, 5, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(person_count[:15])\n",
    "# np.array(kpoints).shape\n",
    "# np.array(openpose2deeppose(keypoint_array[0])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Save data into HDF5 - change batch_size and image_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening images\n",
      "167 184 (17, 19, 2) [3, 1, 1, 7, 5]\n",
      "Creating a new hdf5 file\n",
      "10 (17, 1080, 1920, 3) (17, 19, 2) (17,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "184 211 (27, 19, 2) [7, 9, 4, 4, 3]\n",
      "Creating a new hdf5 file\n",
      "11 (27, 1080, 1920, 3) (27, 19, 2) (27,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "211 223 (12, 19, 2) [3, 1, 3, 3, 2]\n",
      "Creating a new hdf5 file\n",
      "12 (12, 1080, 1920, 3) (12, 19, 2) (12,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "223 244 (21, 19, 2) [5, 5, 5, 4, 2]\n",
      "Creating a new hdf5 file\n",
      "13 (21, 1080, 1920, 3) (21, 19, 2) (21,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "244 262 (18, 19, 2) [2, 4, 3, 6, 3]\n",
      "Creating a new hdf5 file\n",
      "14 (18, 1080, 1920, 3) (18, 19, 2) (18,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "262 285 (23, 19, 2) [6, 4, 5, 4, 4]\n",
      "Creating a new hdf5 file\n",
      "15 (23, 1080, 1920, 3) (23, 19, 2) (23,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "285 296 (11, 19, 2) [2, 1, 3, 4, 1]\n",
      "Creating a new hdf5 file\n",
      "16 (11, 1080, 1920, 3) (11, 19, 2) (11,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "296 312 (16, 19, 2) [3, 2, 3, 5, 3]\n",
      "Creating a new hdf5 file\n",
      "17 (16, 1080, 1920, 3) (16, 19, 2) (16,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "312 326 (14, 19, 2) [2, 1, 3, 4, 4]\n",
      "Creating a new hdf5 file\n",
      "18 (14, 1080, 1920, 3) (14, 19, 2) (14,)\n",
      "\n",
      "\n",
      "Opening images\n",
      "326 347 (21, 19, 2) [4, 9, 5, 1, 2]\n",
      "Creating a new hdf5 file\n",
      "19 (21, 1080, 1920, 3) (21, 19, 2) (21,)\n",
      "\n",
      "\n",
      "Writing metadata\n",
      "Completed in 117.45430946350098\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf \"./$OUT_DIR\"\n",
    "# !rm -rf \"./$META_DIR\"\n",
    "# !rm -rf \"./$JSON_DIR\"\n",
    "# !mkdir \"./$OUT_DIR\"\n",
    "# !mkdir \"./$META_DIR\"\n",
    "# !mkdir \"./$JSON_DIR\"\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# output directory\n",
    "OUT_DIR = 'output'\n",
    "\n",
    "# metadata directory - store input image paths\n",
    "META_DIR = 'metadata'\n",
    "\n",
    "# json directory\n",
    "JSON_DIR = 'json'\n",
    "\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.mkdir(OUT_DIR)\n",
    "    \n",
    "if not os.path.isdir(META_DIR):\n",
    "    os.mkdir(META_DIR)\n",
    "    \n",
    "if not os.path.isdir(JSON_DIR):\n",
    "    os.mkdir(JSON_DIR)\n",
    "    \n",
    "\n",
    "# number of (image,keypoint) pairs to save at a time\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# number of image,keypoint) pairs to save in a hdf5 file\n",
    "# IMAGE_PER_FILE has to be greater than or equal to BATCH_SIZE\n",
    "IMAGE_PER_FILE = 5\n",
    "\n",
    "# if to create a new hdf5 file\n",
    "INITIALIZE_HDF5 = True\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    count = 50 # set this to starting value of i * BATCH_SIZE to resume from middle files\n",
    "    hf = None\n",
    "    \n",
    "    for i in range(10, 20):#len(image_paths)//BATCH_SIZE + 1):\n",
    "        print('Opening images')\n",
    "        # list of image arrays\n",
    "        image_files_batch = np.array([file for file in image_files[BATCH_SIZE*i : BATCH_SIZE*(i+1)]])\n",
    "        image_paths_batch = np.array([image_path for image_path in image_paths[BATCH_SIZE*i : BATCH_SIZE*(i+1)]])\n",
    "        image_array_batch = np.array([np.array(Image.open(image_path)) \n",
    "                                for image_path in image_paths_batch])\n",
    "\n",
    "        '''\n",
    "        directly indexing keypoints_deeppose will result in lesser keypoints\n",
    "        there are multiple keypoints corresponding to a single image\n",
    "        \n",
    "        person_count = [2, 3, 1, 4, 3, 2, 3, 1, 2]\n",
    "        person_count_cum = [2, 5, 6, 10, 13, 15, 18, 19, 21]\n",
    "        \n",
    "        i = 0\n",
    "        batch size = 3\n",
    "        keypoints from 1 to 6\n",
    "        \n",
    "        i = 1\n",
    "        batch size = 3\n",
    "        keypoints from (6+1) to 15\n",
    "        \n",
    "        i = 2\n",
    "        batch size = 3\n",
    "        keypoints from (15+1) to 21\n",
    "        '''\n",
    "        if i == 0:\n",
    "            start_idx = 0\n",
    "            end_idx = person_count_cum[BATCH_SIZE-1] \n",
    "        else:\n",
    "            start_idx = person_count_cum[BATCH_SIZE*i - 1]\n",
    "            end_idx = person_count_cum[BATCH_SIZE*(i+1) - 1]\n",
    "            \n",
    "        keypoints_deeppose_batch = np.array(keypoints_deeppose[start_idx : end_idx])\n",
    "\n",
    "        person_count_batch = person_count[BATCH_SIZE*i : BATCH_SIZE*(i+1)]\n",
    "\n",
    "        print(start_idx, end_idx, keypoints_deeppose_batch.shape, person_count_batch)\n",
    "        \n",
    "        # repeat image according to the number of person it has\n",
    "        image_array_repeat_batch = np.repeat(image_array_batch, person_count_batch, axis=0)\n",
    "        image_files_repeat_batch = np.repeat(image_files_batch, person_count_batch, axis=0)\n",
    "\n",
    "        # save images and keypoints\n",
    "        if ((count) % IMAGE_PER_FILE) == 0:\n",
    "            # close opened file\n",
    "            if hf:\n",
    "                with open(f'{META_DIR}/metadata_annotations_{file_index}.txt', 'a') as f:\n",
    "                    f.write(str(list((hf[\"image_path\"]))))\n",
    "                    \n",
    "                    \n",
    "#                 annotations_consolidated = get_annotations_image(hf['annotations'], \n",
    "#                                                                  person_count[count-IMAGE_PER_FILE : count], \n",
    "#                                                                  image_files_batch[count-IMAGE_PER_FILE : count])\n",
    "                \n",
    "#                 with open(f'{JSON_DIR}/annotations_{file_index}.txt', 'a') as f:\n",
    "#                     f.write(str(annotations_consolidated))\n",
    "#                     f.write(json.dumps(str(list((hf[\"annotations\"])))))\n",
    "#                     f.write(str(list((hf[\"annotations\"]))))\n",
    "                    \n",
    "                hf.close()\n",
    "                \n",
    "            print('Creating a new hdf5 file')\n",
    "            \n",
    "            # open file for saving data\n",
    "            file_index = count // IMAGE_PER_FILE\n",
    "            hf = h5py.File(f'{OUT_DIR}/openpose_annotations_{file_index}.h5', 'a')\n",
    "            \n",
    "            # maxshape should be (None,) to be able to extend the file\n",
    "            # chunks let the file chunks be stored in diff parts of the disk\n",
    "            hf.create_dataset('images', data=image_array_repeat_batch, compression='gzip', compression_opts=9, \n",
    "                              chunks=True, maxshape=(None,*image_array_repeat_batch.shape[1:]))\n",
    "            hf.create_dataset('annotations', data=keypoints_deeppose_batch, compression='gzip', compression_opts=9, \n",
    "                              chunks=True, maxshape=(None,*keypoints_deeppose_batch.shape[1:]))\n",
    "            \n",
    "            # hdf5 can't store list of strings \n",
    "            path_list = np.array(image_files_repeat_batch, dtype='S')\n",
    "            hf.create_dataset('image_path', data=path_list, compression='gzip', compression_opts=9, \n",
    "                              chunks=True, maxshape=(None,))\n",
    "#             print(path_list)\n",
    "        else:\n",
    "            print('Extending hdf5 file with new data')\n",
    "            \n",
    "            hf[\"images\"].resize((hf[\"images\"].shape[0] + image_array_repeat_batch.shape[0]), axis = 0)\n",
    "            hf[\"images\"][-image_array_repeat_batch.shape[0]:] = image_array_repeat_batch\n",
    "\n",
    "            hf[\"annotations\"].resize((hf[\"annotations\"].shape[0] + keypoints_deeppose_batch.shape[0]), axis = 0)\n",
    "            hf[\"annotations\"][-keypoints_deeppose_batch.shape[0]:] = keypoints_deeppose_batch\n",
    "                              \n",
    "            # hdf5 can't store list of strings \n",
    "            #path_list = np.char.encode(np.array(image_paths_batch, dtype='U'), encoding='utf8')\n",
    "            path_list = np.array(image_files_repeat_batch, dtype='S')\n",
    "            hf[\"image_path\"].resize((hf[\"image_path\"].shape[0] + path_list.shape[0]), axis = 0)\n",
    "            hf[\"image_path\"][-path_list.shape[0]:] = path_list\n",
    "                \n",
    "        count += BATCH_SIZE\n",
    "        print(f\"{i:} {hf['images'].shape} {hf['annotations'].shape} {hf['image_path'].shape}\\n\\n\")\n",
    "              \n",
    "    # save data on exit\n",
    "    if hf:\n",
    "        with open(f'{META_DIR}/metadata_annotations_{file_index}.txt', 'a') as f:\n",
    "            print('Writing metadata')\n",
    "            f.write(str(list((hf[\"image_path\"]))))\n",
    "\n",
    "#         annotations_consolidated = get_annotations_image(list(hf['annotations']), \n",
    "#                                                              person_count[count-BATCH_SIZE : count], \n",
    "#                                                              image_files_batch[count-BATCH_SIZE : count])\n",
    "\n",
    "#         with open(f'{JSON_DIR}/annotations_{file_index}.txt', 'a') as f:\n",
    "#             print('writing json')\n",
    "#             f.write(str(annotations_consolidated))\n",
    "\n",
    "        hf.close()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    if hf:\n",
    "        hf.close()\n",
    "    print(f\"Completed in {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation Hotkeys\n",
    "------------\n",
    "* <kbd>+</kbd><kbd>-</kbd> = rescale image by ±10%\n",
    "* <kbd>left mouse button</kbd> = move active keypoint to cursor location\n",
    "* <kbd>W</kbd><kbd>A</kbd><kbd>S</kbd><kbd>D</kbd> = move active keypoint 1px or 10px\n",
    "* <kbd>space</kbd> = change <kbd>W</kbd><kbd>A</kbd><kbd>S</kbd><kbd>D</kbd> mode (swaps between 1px or 10px movements)\n",
    "* <kbd>J</kbd><kbd>L</kbd> = next or previous image\n",
    "* <kbd><</kbd><kbd>></kbd> = jump 10 images forward or backward\n",
    "* <kbd>I</kbd>,<kbd>K</kbd> or <kbd>tab</kbd>, <kbd>shift</kbd>+<kbd>tab</kbd> = switch active keypoint\n",
    "* <kbd>R</kbd> = mark image as unannotated (\"reset\")\n",
    "* <kbd>F</kbd> = mark image as annotated (\"finished\")\n",
    "* <kbd>esc</kbd> or <kbd>Q</kbd> = quit\n",
    "\n",
    "# Annotate data\n",
    "Annotations are saved automatically. \n",
    "The skeleton in each frame will turn blue when the frame is fully annotated. If there are no visible keypoints, this means the frame hasn't been annotated, so try clicking to position the keypoint in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from deepposekit import Annotator\n",
    "from os.path import expanduser\n",
    "import glob\n",
    "HOME = expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run annotator - change datapath argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint statement in /home/nabhanpv/miniconda3/lib/python3.7/site-packages/deepposekit-0.3.5-py3.7.egg/deepposekit/annotate/gui/Annotator.py\\n\\nprint((self.n_images, self.n_keypoints))\\n\\nprint(self.skeleton.loc[:, [\"x\", \"y\"]].shape)\\nprint(h5file[\"annotations\"][self.image_idx].shape)\\n\\nprint(self.skeleton.loc[:, \"annotated\"].shape)\\nprint(h5file[\"annotated\"][self.image_idx].shape)\\n\\nprint(h5file[\\'annotated\\'].shape)\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "print(n)\n",
    "datapath = f'output/openpose_annotations_{n}.h5'\n",
    "app = Annotator(datapath=datapath,\n",
    "            dataset='images',\n",
    "            skeleton='skeleton.csv',\n",
    "            shuffle_colors=False,\n",
    "            text_scale=1.0)\n",
    "\n",
    "n += 1\n",
    "\n",
    "'''\n",
    "print statement in /home/nabhanpv/miniconda3/lib/python3.7/site-packages/deepposekit-0.3.5-py3.7.egg/deepposekit/annotate/gui/Annotator.py\n",
    "\n",
    "print((self.n_images, self.n_keypoints))\n",
    "\n",
    "print(self.skeleton.loc[:, [\"x\", \"y\"]].shape)\n",
    "print(h5file[\"annotations\"][self.image_idx].shape)\n",
    "\n",
    "print(self.skeleton.loc[:, \"annotated\"].shape)\n",
    "print(h5file[\"annotated\"][self.image_idx].shape)\n",
    "\n",
    "print(h5file['annotated'].shape)\n",
    "'''\n",
    "\n",
    "# app = Annotator(datapath=HOME + '/deepposekit-data/datasets/locust/annotation_data_release.h5',\n",
    "#                 dataset='images',\n",
    "#                 skeleton=HOME + '/deepposekit-data/datasets/locust/skeleton.csv',\n",
    "#                 shuffle_colors=False,\n",
    "#                 text_scale=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (22, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (22, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (22, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (23, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (23, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (23, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (24, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (24, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (24, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (25, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (25, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (25, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (26, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (26, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (26, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (27, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (27, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (27, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (28, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (28, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (28, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (29, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (29, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (29, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (30, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (30, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (30, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (31, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (31, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (31, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (32, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (32, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (32, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (33, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (33, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (33, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (34, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (34, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (34, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (35, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (35, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (35, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (36, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (36, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (36, 19)\n",
      "Successfully created new annotation\n",
      "Creating new annotation\n",
      "Annotator._new_annotation(): new image added (37, 1080, 1920, 3)\n",
      "Annotator._new_annoation(): new annotation added (37, 19, 2)\n",
      "Annotator._new_annoation(): new annotation added (37, 19)\n",
      "Successfully created new annotation\n",
      "Saved\n",
      "19.90607030391693\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "app.run()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. H5PY to JSON - change function input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/openpose_annotations_13.h5\n",
      "Store complete\n"
     ]
    }
   ],
   "source": [
    "annotation_from_h5py2json(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'ch03_20190823171617_159.jpg', b'ch03_20190823171617_159.jpg', b'ch03_20190823171617_1059.jpg', b'ch03_20190823171617_1959.jpg', b'ch03_20190823171617_1959.jpg', b'ch03_20190823171617_1959.jpg', b'ch03_20190823171925_124.jpg', b'ch03_20190823171925_124.jpg', b'ch03_20190823172024_466.jpg', b'ch03_20190823172024_466.jpg', b'ch03_20190823172024_466.jpg', b'ch03_20190823172024_466.jpg', b'ch03_20190823172024_466.jpg', b'ch03_20190823172024_466.jpg']\n",
      "(14, 1080, 1920, 3)\n",
      "(14,)\n",
      "(14, 19, 2)\n",
      "(14, 19)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "idx = 1\n",
    "# # ['annotated', 'annotations', 'image_path', 'images', 'skeleton']>\n",
    "with h5py.File(\"output/openpose_annotations_3.h5\", 'r+') as hf:\n",
    "    print(list(hf['image_path']))\n",
    "    print(hf['images'].shape)\n",
    "    print(hf['image_path'].shape)\n",
    "    print(hf['annotations'].shape)\n",
    "    print(hf['annotated'].shape)\n",
    "#     hf['image_path'][:] = p\n",
    "#     print(set(list(hf['image_path'])))\n",
    "#     print(hf['annotations'].shape)\n",
    "#     hf['images'][idx:-1] = hf['images'][idx+1:]\n",
    "#     hf['images'].resize((hf[self.dataset].shape[0] - 1), axis = 0)\n",
    "#     print(hf['images'].shape)\n",
    "#     print(list(hf['image_path']))\n",
    "# #     print(hf['skeleton'].shape)\n",
    "#     print(hf.keys())\n",
    "# #     print(hf['images'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# image_files[:6]\n",
    "\n",
    "path = ['ch03_20190823170038_775.jpg',\n",
    "'ch03_20190823170038_1675.jpg',\n",
    "'ch03_20190823170038_2575.jpg',\n",
    "'ch03_20190823170038_3475.jpg',\n",
    "'ch03_20190823170038_4375.jpg']\n",
    "\n",
    "pc = [6, 10, 8, 7, 9]\n",
    "\n",
    "p = np.array(np.repeat(path, pc, axis=0), dtype='S')\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ch03_20190828191227_3587_rendered.json',\n",
       " 'ch03_20190828191227_2687_rendered.json',\n",
       " 'ch03_20190828191722_67_rendered.json',\n",
       " 'ch03_20190828191227_887_rendered.json']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.listdir('json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('json/ch03_20190828191227_2687_rendered.json') as f:\n",
    "#     j = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(j['people']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
